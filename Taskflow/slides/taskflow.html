<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<meta name="description" conent="An introduction to Taskflow, the C++ task graph library.">
		<meta name="author" content="Martin Nilsson">

		<title>Taskflow</title>


		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">


				<!-- PART I -->
				<!-- INTRODUCTION -->


				<section>
					<h1 class="r-fit-text">Taskflow - A Task Graph Library</h1>
					<p>Subtitle?</p>
					<p>A short introduction</p>
					<p>Parallel programs made easy</p>
					<aside class="notes">
						I'm gonna be talking about computing, because I like computing.<br><br>
						This is a presentation about Taskflow, the task graph library, and why we need such a thing.
					</aside>
				</section>

				<section>
					<h3>The walls</h3>
					<img src="images/cpu-trend.jpg">
					<small><a href="https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/">https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/</a></small>

					<aside class="notes">
						For a while computer users enjoyed a steady pace of speed increases.<br><br>
						This kept going until the early 2000's when we hit the walls.<br><br>
						The power and frequency wall in particular.<br><br>
						These are joined by the so called Dennard scaling that says that as the transistors get smaller they use less energy so that the energy density, heat per area, remains roughly constant even though the number of transistors increases.<br><br>
						This stopped being the case somwhere around here, and since peak frequency is highly dependent on the voltage we can push through the transistors and heat is highly depenent on the voltage we hit a point where increasing the frequency and adding more and more tiny transistors lead to unmangeable heat generation.<br><br>
						The famous Moore's law still doing alright but single-threaded performance has not keeping up with the number of transistors and have started to level off.<br><br>
						Frequency not increasing much anymore and CPU / compiler makers are having trouble extracting more instruction level parallelism from straightline code.<br><br>
						To combat this we got more cores.<br><br>
						This means that those of us who remember the times where a new computer would make all programs run faster had to get acustomed to that not being the case anymore and we had to start working to make use all those new transistors hiding behind the second, and third, and ninetysixth core.<br><br>
						The free lunch as over, as the saying goes.
					</aside>
				</section>


				<section>
					<h3>Ways of writing multi-threaded programs</h3>
					<ul>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Manual threads</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Data parallel</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Producer / consumer</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Pipeline</p></li>
						<li><p>Task graph</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Actors / events</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Fork-join</p></li>
					</ul>
					<aside class="notes">
						Multiple cores means that we can no longer rely on single-threaded programs, we need to go parallel or concurrent.
						<br><br>
						These mean different things, but for this discussion I'm going to ignore that.
						<br><br>
						Think multi-threaded and we sort of cover what we need.
						<br><br>
						Here are some ways of organizing a multi-threaded program.
						<br><br>
						These are not clear-cut implementation strategies but broad categories.
						<br><br>
						Manual threads means starting a new thread for each task that needs to be done. Think one for audio, one for physics, one for AI, one for graphics, one for IO, and so on in a video game.
						<br><br>
						Data parallel means that we have a serial main thread and whenever we have a large set of elements we need to perform the same operation on then we bring in all the other cores to chrunch the numbers. This is how OpenMP and GPUs work. The classical parallel-for concept.
						<br><br>
						Producer / consumer can be used when we have one part of the program that through some process produces data and another part that reads that data and computes an output. If these are indenpendent then they can be run in parallel on different threads.
						<br><br>
						A pipeline can be though of a chain of producer / consumer pairs where each thread is both a consumer of its input and a producer of the output that becomes the input for the next thread.
						<br><br>
						A task graph is kinda like a branching pipeline where the output produced by one task may be consumed by multiple later tasks. In a task graph we rarely have one thread per task but a pool of worker threads that can be assigned any task that becomes ready to run. A task is typically short-lived while a pipeline is a steady-state construction where data flows through continuously.
						<br><br>
						The actor model models a system of indenpendent concurrently executing entities that listen for events and responds to them with some processing and possibly the generation of new events.
						<br><br>
						Fork-join is a broad categorization that means that that is a main flow of logic that occasionally branches out to parallel computation and that the result from those parallel computations are collected back before the main flow continues. The data parallel model is an example of fork-join.
						<br><br>
						It is possible to combine these, which is what AGX Dynamcis does.
						<br><br>
						Today the focus will be on task graphs, which forms the basis for parallelization in AGX Dynamics and what the Taskflow library provides.
						<br><br>
						Taskflow also does GPU stuff but I'm not going to go into that part today.<br><br>
					</aside>
				</section>

				<section>
					<h3>Task Graph Computing System </h3>
					<p>Instead of chaining function calls together as the program is executed the structure of the computation is defined up-front in a task graph.</p>

					<aside class="notes">
						The "up-front" part is not as rigid as it may sound may expressed like this.
					</aside>
				</section>

				<section>
					<h3>Task Based Parallelism</h3>
					<p>Construct a dependency graph with tasks as nodes.</p>
					<p>Dependencies between tasks represented as edges.</p>
					<p>Execute tasks top-to-bottom.</p>
					<p>Exploit task independence for parallelism.</p>
				</section>

				<section>
					<h3>Example Task Graph</h3>
					<img src="./images/example_task_graph.svg">
					<p>Once Task 1 has completed we can run Task 2 and 3 in parallel.</p>
				</section>

				<section>
					<h3>Static Tasking </h3>
					<p>The simplest form of task graph.</p>
					<p>All tasks are known up front.</p>
					<p>The example on the previous slide is a static task graph.</p>

					<aside class="notes">
						There are different types of task graphs with different types of expressive power.
						<br><br>
						Static tasking is the simplest form. When I said "up-front" earlier this is what I was talking about.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Fundamental Building Blocks</h3>
					<p data-id="task">A task</p>
					<p data-id="graph">A task graph</p>
					<p data-id="runtime"> A runtime</p>

					<aside class="notes">
						Let's get technical.
						<br><br>
						To realize all of this in an actual application we need three things.
						<br><br>
						Something to represent the tasks to execute. This includes both the work to be performed and metadata such as name, user data and type.
						<br><br>
						Something to represent the dependencies between tasks, the dependency graph.
						<br><br>
						A runtime that can run all tasks in the correct order.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Fundamental Building Blocks</h3>
					<p data-id="task">A task</p><code>tf::Task</code>
					<br><br>
					<p data-id="graph">A task graph</p> <code>tf::Taskflow</code>
					<br><br>
					<p data-id="runtime">A runtime </p> <code>tf::Executor</code>

					<aside class="notes">
						Taskflow is a C++ library and uses classes to represent these things.
						<br><br>
						All placed within the tf namespace, short for Taskflow.
						<br><br>
						Let's create and run the simplest possible task graph.
					</aside>
				</section>


				<!-- PART II -->
				<!-- SIMPLEST POSSIBLE TASK GRAPH -->


				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="1-4">
						void work()
						{
							std::cout << "Doing work.\n";
						}
					</code></pre>
					<aside class="notes">
						To create a task we need to start with some work to do.
						<br><br>
						Here that work is to print the string "Doing work.".
						<br><br>
						You can imagine any other type of work one might need to do.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="6-8">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
						}
					</code></pre>
					<aside class="notes">
						With some work ready to be performed we need to do some setup to arrange for the work to be performed.
						<br><br>
						In this example we do this in main.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="8">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Taskflow taskflow;
						}
					</code></pre>
					<aside class="notes">
						We create an instance of the Taskflow class to get a task graph in which we can create our task that will call the work function.
					</aside>
				</section>


				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="9">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
						}
					</code></pre>
					<aside class="notes">
						To create a task we don't directly create an instance of the task class and add to the taskflow.
						<br><br>
						Instead we let the taskflow create the task for us by calling emplace.
						<br><br>
						The parameter to emplace is the code we want to have executed when the task is run. This can be anything that is callable with no arguments. Here we use a free function but it could just as well have been a lambda funciton or an object with a call operator.
						<br><br>
						The thing we get back is not the actual task but a handle to it. A handle that is small, cheap to pass around, and safe to copy. It's a pointer, basically. The actual data lives as a node within the task graph.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="8">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
						}
					</code></pre>
					<aside class="notes">
						With the task graph in place we are ready to run it and for this we need the runtime, which in Taskflow is called an executor.
						<br><br>
						In my example I always put it high in main to signal that this is a heavy-weigth, long-lived, and shared resource. It is not meant to be created every time we want to run a task graph. Instead we should create one at startup and reuse it throught the application's life time.
						<br><br>
						This is where the worker threads are spawned.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="11">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
						}
					</code></pre>
					<aside class="notes">
						Now we are ready to do some work for real.
						<br><br>
						Executor.run will cause the runtime to start running tasks using the worker threads. We can chose to either keep doing other things in the main thread or wait for the tasks to finish executing. Since we have nothing else to do in this example we wait.
						<br><br>
						I have not yet found a way to make the main thread participate in the task work, which is a bit limiting given that in AGX Dynamics we always run user supplied event listeners in the main thread. There is more to Taskflow that I haven't explored yet it is possible that this can be solved.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers>
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
						}
					</code></pre>
					<aside class="notes">
						And there you have it, the smallest possible task graph in Taskflow.
						Though we're missing some header inclues.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="1,2"><script type="text/template">
						#include "taskflow/taskflow.hpp"
						#include <iostream>

						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
						}
					</script></code></pre>
					<aside class="notes">
						This can compile and run. But we also want pretty picture, like the task graph example we saw in the introduction.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="16-19"><script type="text/template">
						#include "taskflow/taskflow.hpp"
						#include <iostream>
						#include <fstream>

						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
							taskflow.name("Simplest Possible Task Graph");
							task.name("A Lonely Task");
							std::ofstream file("simplest_possible_task_graph.dot");
							taskflow.dump(file);
						}
					</script></code></pre>
					<aside class="notes">
						Let's give the graph and task a name and dump them to a DOT file. It won't be all that useful for this minimal example but is a great help when build larger real-world graphs.
					</aside>
				</section>

				<section>
					<h3>SIMPLEST POSSIBLE TASK GRAPH</h3>
					<img src="./images/simplest_possible_task_graph.svg">
					<aside class="notes">
						And this is what it looks like, after some edits to get a dark-mode variant.
						<br><br>
						But sometimes we need more than one task, so let's make a new example where we create a few of them
					</aside>
				</section>


				<!-- PART III -->
				<!-- TASK DEPENDENCIES -->


				<section>
					<h3>TASK DEPENDENCIES</h3>
					<pre><code class="hljs c++" data-trim data-line-numbers="|2|1|3-7|3|5|3-5"><script type="text/template">
						auto [t1, t2, t3, t4, t5, t6] = taskflow.emplace(
							[](){}, [](){}, [](){}, [](){}, [](){}, [](){});
						t1.precede(t2, t3);
						t3.precede(t4, t5);
						t6.succeed(t2, t4, t5);
					</script></code></pre>
					<p class="fragment fade-in"><img src="./images/example_task_graph.svg"></p>
					<aside class="notes">
						From now on I won't show entire code examples since they won't fit on a slide anymore. Instead I'll show only enough to get the point accross.
						<br><br>
						I'll also use an empty lambda whenever the work done doesn't matter for the presentation. Here we have a whole bunch of them being passed to emplace.
						<br><br>
						The effect of this is that we get the same number of tasks handles back in a tuple, which we can use structured bindings to unpack into a  bunch of variables, here named t1, t2, and so on up to t6.
						<br><br>
						Dependencies are created using the precede and succeed Task member functions. precede makes the task on the left precede, i.e. happen before, the tasks on the right. succeed, on the other hand, makes the task happen after the parameter tasks.
						<br><br>
						Looking at the resuling dependency graph we get this familiar diagram from the 	start of the presentation.
						<br<br>
						It doesnt matter if we do t1.precede(t2) or t2.succeede(t1), both variants describe the same dependency. The only difference is that it is easy to pass many parameters to a single function so when a node has more input dependencies than output then I tend to use succeed and when it has more output dependencies then I tend to use precede.
					</aside>
				</section>


				<!-- PART IV -->
				<!-- TASKFLOW BACKGROUND -->


				 <section>

				 </section>


				<!-- PART V -->
				<!-- Dynamic Tasks -->


				<section>
					<h3>DYNAMIC TASKS</h3>
					<p>Tasks that might not exist.</p>
					<aside class="notes">
						We don't always know ahead of time whether a particular task will need to be run. For this case Taskflow provides dynamics tasks. A dynamic task is one that may contain child tasks and those tasks are created at runtime when the parent task is run. This makes it possible for earlier tasks to control what later tasks does.
						<br><br>
						Let's first look at a trivial example, and then one a smitten closer to a real-world use-case.
					</aside>
				</section>

				<section>
					<h3>Dynamic tasks</h3>
					<pre><code class="hljs c++" data-trim data-line-numbers><script type="text/template">
						tf::Task parent = taskflow.emplace(
							[](tf::Subflow& subflow) {
								tf::Task child = subflow.emplace([]() {});
							});
					</script></code></pre>
					<aside class="notes">
						We tell Taskflow that a task is dynamic by providing a callback that takes a Subflow parameter. A subflow is similar to a taskflow in that we can emplace new tasks in it. The tasks we get back are just like any other and we can give then name add depdencies, and so on. We can create any number of child tasks, and the child tasks can themselves be dynamic tasks.
						<br><br>
						What's important to realize is that while tasks created on a taskflow object are  created before the task graph is executed, the tasks in the sublow are created while execution is ongoing. Some tasks may already have finished by the time the parent task is executed and the child task created.
						<br><br>
						Another important thing is that the child task is run immediately after the parent callback returns. This means that any task that has a dependency on the parent task also has an transitive dependency on the child task.
						<br><br>
						The child task is "inside" the parent task.
					</aside>
				</section>

				<section>
					<h3>DYNAMIC TASKS</h3>
					<pre><code class="hljs c++" data-trim data-line-numbers="|1|4|6,7|8-10|13-15|18-26|29-34|36-44|46-48|50-57"><script type="text/template">
						class Space
						{
						public:
							void emplaceTasks(tf::Taskflow& taskflow);

							void broadPhase();
							void nearPhase(tf::Subflow& subflow);
							void nearPhaseSphereSphere();
							void nearPhaseSphereBox();
							void nearPhaseBoxBox();

						private:
							int m_numSphereSperePairs {0};
							int m_numSphereBoxPairs {0};
							int m_numBoxBoxPairs {0};
						};

						void Space::emplaceTasks(tf::Taskflow& taskflow)
						{
							tf::Task broadPhaseTask = taskflow.emplace(
								[this]() { broadPhase(); });

							tf::Task nearPhasesTask = taskflow.emplace(
								[this](tf::Subflow& subflow) { nearPhase(subflow); });

							broadPhaseTask.precede(nearPhasesTask);
						}

						void Space::broadPhase()
						{
							m_numSphereSperePairs = 1;
							m_numSphereBoxPairs = 0;
							m_numBoxBoxPairs = 1;
						}

						void Space::nearPhase(tf::Subflow& subflow)
						{
							if (m_numSphereSperePairs > 0)
								subflow.emplace([this]() { nearPhaseSphereSphere(); });
							if (m_numSphereBoxPairs > 0)
								subflow.emplace([this]() { nearPhaseSphereBox(); })
							if (m_numBoxBoxPairs > 0)
								subflow.emplace([this]() { nearPhaseBoxBox(); })
						}

						void Space::nearPhaseSphereSphere(){}
						void Space::nearPhaseSphereBox(){}
						void Space::nearPhaseBoxBox(){}

						int main()
						{
							Space space;
							tf::Executor executor;
							tf::Taskflow taskflow;
							space.emplaceTasks(taskflow);
							executor.run(taskflow).wait();
						}

					</script></code></pre>
					<aside class="notes">
						Let's move to a larger example, one that well need to page through bit by bit. I'm using a somewhat more real-world example here, with a two-phase collision detection implementation.
						<br><br>
						We have a class named Space that handles all collision-detection tasks.
						<br><br>
						It has en emplaceTasks function that is kind analogus to our current createUpdateTask.
						<br><br>
						This function will create two tasks, one for broad-phase and one for narrow-phase. The broadPhase function is just like all the other work callbacks we've seen: a function with no parameters.
						In our example it finds all shape pairs that are close to each other and groups them by shape types.
						The nearPhase function is different. It takes a Subflow parameter. This is what tells Taskflow that this task is not a static task but a dynamic one. A task that can create internal child tasks.
						<br><br>
						The near-phase task will create a child task for every shape type pair, but only if the broad-phase found at least one shape pair with those types.
						<br><br>
						Space has some internal state. Here we simplify it to just home may shape pairs there are for each type-pair. It is the responsibility of broad-phase to update these values.
						<br><br>
						Let's have a look at emplaceTasks. It's pretty simple. It creates two tasks in the given Taskflow, one for broad-phase and one for near-phase. We could have placed the entire implementation of these two functions within the lambdas, but I find it more readable to keep the implementation in their own functions and have the callback we give to Taskflow just call the  class member function, passing on the parameters. Notice that when creating the near-phase task we pass a lambda that takes a Subflow parameter, which we pass on to the member function. Lastly we make sure the narrow-phase task isn't allowed to start until the broad-phase task has completed.
						<br><br>
						The broadPhase implementation is just a placeholder here. Pretend we found some  sphere-sphere pairs and some box-box pairs, but no sphere-box pairs.
						<br><br>
						Here comes the interesting bit. The parent task of the near-phase subflow checks the result of the broad-phase and creates only the tasks that have anything to do. The others are simply skipped. Rember that in our example we have sphere-sphere and box-box pairs but no sphere-box pair.
						<br><br>
						The actual near-phase computation we don't care about here.
						<br><br>
						The implementation of main shouldn't present any surprises. We create an instance of our Space class, an executor and a Taskflow as we always do, let the Space instance create it's tasks in the Taskflow and then pass it to the executor to run it.
					</aside>
				</section>

				<section>
					<h3>Dynamic Tasks</h3>
					<img src="images/work_if_needed.svg">
					<aside class="notes">
						The result of all this is a task graph with a broad-phase static task and a near-phase subflow with two near-phase child tasks. Notice that here there is no sphere-box task in the near-phase subflow. In a working simulation the set of tasks could change from time-step to time-step.
						<br><br>
						This is an important point to be aware of. The child tasks doesn't exist until they are creaatd during task graph execution so there is no way to create one of these visualizations of the entire task graph with all possible child tasks. The visualization only captures one specific execution of the task graph.
						<br><br>
						Also, the child tasks are by default cleared at the end of the task execution to avoid wasting memory which means they won't show up at all. To prevent that we must call 'retain(true)' on the subflow instance.
					</aside>
				</section>

				<section>
					<h3>Dynamic Tasks</h3>
					<pre><code class="hljs c++" data-trim data-line-numbers="|22-33|28-31|6-16|3-4|18,19"><script type="text/template">
					int spawn(int n, tf::Subflow& subflow)
					{
						if (n < 2)
							return n;

						int result1;
						tf::Task task1 = subflow.emplace(
							[n, &result1](tf::Subflow& subsubflow) {
								result1 = spawn(n - 1, subsubflow);
							});

						int result2;
						tf::Task task2 = subflow.emplace(
							[n, &result2](tf::Subflow& subsubflow) {
								result2 = spawn(n - 2, subsubflow);
							});

						subflow.join();
						return result1 + result2;
					}

					int main()
					{
						tf::Executor executor;
						tf::Taskflow taskflow;
						int n {5};
						int result {0};
						tf::Task task = taskflow.emplace(
							[n, &result](tf::Subflow& subflow) {
								result = spawn(n, subflow);
							});
						executor.run(taskflow).wait();
					}
					</script></code></pre>
					<aside class="notes">
						Since a subflow is basically the same as a taskflow we can put dynamic tasks inside a dynamic task, creating recursively hirarchical subflows. This way we can implement recursive algorithms. The classical example is the Fibonacci sequence. Let's scroll down to main and look at that first.
						<br><br>
						Specifically when we create the root task.
						<br><br>
						We create a single task from a lambda that captures the n and result variables and when run forwards those to the spawn function.
						<br><br>
						The spawn function does pretty much the same thing, but twice and with smaller numbers for n.
						<br><br>
						And with a base case.
						<br><br>
						And finally returns the sum of the sub-results after joining with the child tasks.
					</aside>
				</section>

				<section>
					<h3>Dynamic tasks</h3>
					<img src="images/fibonacci.svg">
					<aside class="notes">
						The result is a tree structure with a lot of repeated calculations. This is not the best way to compute Fibonacci numbers, but it is a good way to demonstrate recursive algorithms. And creating a whole bunch of subflows isn't the best way to do small-scale parallelism like this, the Taskflow library provides the Runtime class for this, which we may get to later if time allows. For now let's do some branching instead, another way to do control what tasks get to run.
					</aside>
				</section>


				<!-- PART VI -->
				<!-- BRANCHING -->


				<section>
					<h3>Branching</h3>
					<p>A way to control where to go next.</p>
					<aside class="notes">
						At it's most basic level a branch in Taskflow is a task that choses which of its outgoint dependencies, i.e. which of the tasks that is precedes, that should be allowed to run. It is kinda like a switch statment. Let's look at an example.
					</aside>
				</section>

				<section>
					<h3>Branching</h3>
					<img src="images/branch.svg">
					<aside class="notes">
						A branch node is visalized as a diamond and its outgoing dependency edges as dashed lines instead of solid. This means that they are weak dependencies. We'll get more into what this means later, for now let's focus on the numbers next to the dashed lines. This is how the branch task decides which task should be run next. The thing that defines a branch task in Taskflow is that the callback we give the task has an integer return value, and the value returned indentifies which outgoing dependency should be followed. Let's look at some code.
					</aside>
				</section>

				<section>
					<h3>Branching</h3>
					<pre><code class="hljs c++" data-trim data-line-numbers="|1|3-6|8-10|12-14|16-18|25-28|30-31"><script type="text/template">
						static int number {0};

						void read_data() {
							std::cout << "Data? ";
							std::cin >> number;
						}

						int even_or_odd() {
							return number % 2;
						}

						void print_even() {
							std::cout << "Number " << number << " is even.\n";
						}

						void print_odd() {
							std::cout << "Number " << number << " is odd.\n";
						}

						int main()
						{
							tf::Taskflow taskflow;
							tf::Executor executor;

							tf::Task read_data = taskflow.emplace(::read_data);
							tf::Task even_or_odd = taskflow.emplace(::even_or_odd);
							tf::Task print_even = taskflow.emplace(::print_even);
							tf::Task print_odd = taskflow.emplace(::print_odd);

							read_data.precede(even_or_odd);
							even_or_odd.precede(print_even, print_odd);

							executor.run(taskflow).wait();
							return 0;
						}
						</script></code></pre>

						<p class="fragment fade-in"><img src="./images/branch.svg"></p>

						<aside class="notes">
							We have a single piece of shared state, a number to determine whether it is even or odd.
							<br><br>
							We ask the user to provide the number, so that we can test with different values.
							<br><br>
							This is the branch task callback, a function that returns 0 if the number is even and 1 if it is odd. We must remember this mapping since it will become important later when we set up the precede list for the branch task.
							<br><br>
							A function to print even numbers.
							<br><br>
							And a function to print odd numbers.
							<br><br>
							In main we create a task for each of the functions listed above,
							<br><br>
							and set up a dependency from read_data to even_or_odd, so that the number is available by the time we need to look at it. We also add to outgoing dependencies to even_or_odd, the first, at index 0, being print_even, and the second, at index 1, being print_odd. This must match the return value from even_or_odd, it's return value is an index into this list.
							<br><br>
							The result is that we get this dependency graph.
						</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Branching</h3>
					<p data-id="sub-title">Weak dependencies can lead back up to earlier tasks.</p>
					<p data-id="graph" class="fragment fade-in"><img src="./images/branch.svg"></p>
					<aside class="notes">
						Let's consider the weak dependencies again, and how they differ from the regular strong dependencies. In the regular case all input dependencies must be fullfilled before a task is run. For example, Even Or Odd cannot start until Read Data has completed because there is a strong dependency between them. Weak dependencies are different in that the sucesssor task is run immediately when a weak dependency is fullfilled, regardless of any other dependencies. Even if that task has already been run, in which case it, and all its sucessors, will be run again. This means that we can create loops in our task graph.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Branching</h3>
					<p data-id="sub-title">Weak dependencies can lead back up to earlier tasks.</p>
					<p data-id="graph"><img src="./images/counter.svg"></p>
					<aside class="notes">
						Here is an example of a simple loop that increments a counter to a user-provided goal. We start by reading the goal and the repeatedly increment the counter until the goal is reached and we're done.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Branching</h3>
					<p data-id="sub-title">Weak dependencies can lead back up to earlier tasks.</p>
					<pre data-id="code"><code class="hljs c++" data-trim data-line-numbers="|1-2|4-19|26-30|32-35"><script type="text/template">
						static int counter {0};
						static int goal {10};

						void read_goal() {
							std::cout << "Goal? ";
							std::cin >> goal;
						}

						void increment_counter() {
							++counter;
						}

						void print_counter() {
							std::cout << counter << " / " << goal << std::endl;
						}

						int is_goal_reached() {
							return counter == goal;
						}

						int main()
						{
							tf::Taskflow taskflow;
							tf::Executor executor;

							tf::Task read_goal = taskflow.emplace(::read_goal);
							tf::Task increment_counter = taskflow.emplace(::increment_counter);
							tf::Task print_counter = taskflow.emplace(::print_counter);
							tf::Task is_goal_reached = taskflow.emplace(::is_goal_reached);
							tf::Task done = taskflow.emplace([](){});

							read_goal.precede(increment_counter);
							increment_counter.precede(print_counter);
							print_counter.precede(is_goal_reached);
							is_goal_reached.precede(increment_counter, done);

							executor.run(taskflow).wait();
							return 0;
						}
					</script></code></pre>

					<aside class="notes">
						As before we use global variables for any shared data.
						<br><br>
						The work functions are nothing complicated. Notice that the is_goal_reached function returns an integer, making it a branch task.
						<br><br>
						As before we create one task per work function. We also create a done task that does nothing since the branch task must have somewhere to go when it's not looping back up. Taskflow provides placeholder tasks that is a better way to achive the same thing.
						<br><br>
						Most of the dependencies from a simple precede chain. The only exception is is_goal_reached that can either loop back up to increment_counter or fall down to done, which concludes the task graph.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Branching</h3>
					<p>Weak dependencies can lead back up to earlier tasks.</p>
					<pre data-id="code"><code class="hljs c++" data-trim data-ln-start-from="32" data-line-numbers><script type="text/template">
						read_goal.precede(increment_counter);
						increment_counter.precede(print_counter);
						print_counter.precede(is_goal_reached);
						is_goal_reached.precede(increment_counter, done);
					</script></code></pre>

					<p data-id="graph"><img src="./images/counter.svg"></p>

					<aside class="notes">
						Looking at the precede calls next to the resulting task graph gives and overview of what happens. Notice that when is_goal_reached returns 0 we loop back up to increment counter.
						<br><br>
						If the number of iterations is known at compile time then we may believe that we can skip the Real Goal task and have Increment Counter as our root task, but let's see what happens when we do that.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Branching</h3>
					<p>Weak dependencies can lead back up to earlier tasks.</p>
					<pre data-id="code"><code class="hljs c++" data-trim data-ln-start-from="32" data-line-numbers><script type="text/template">
						increment_counter.precede(print_counter);
						print_counter.precede(is_goal_reached);
						is_goal_reached.precede(increment_counter, done);
					</script></code></pre>

					<p data-id="graph"><img src="./images/failed_loop.svg"></p>

					<aside class="notes">
						This may look OK since Increment Counter doesn't have any strong dependencies and we may expect it to start running as soon as the task graph is given to the executor. However, Taskflow only considers tasks with no dependencies at all, not even weak ones, to be root tasks. So this will do nothing. Taskflow will enqueue zero tasks, see that the queue is empty, and return from executor.run immediately. In larger systems with many sub-graphs within a single task graph this is important to keep in mind since it can result in rarely needed work not being done when it should.
					</aside>
				</section>


				<!-- PART VII -->
				<!-- GAUSS-SEIDEL -->


				<section data-auto-animate data-auto-animate-id="gauss-seidel">
					<h3 data-id="title-gauss">Gauss-Seidel</h3>
					<p class="fragment fade-in" data-id="subtitle-gauss">We now have enough knowledge to write a Gauss-Seidel solver.</p>
				</section>

				<section data-auto-animate data-auto-animate-id="gauss-seidel">
					<h3 data-id="title-gauss">Gauss-Seidel</h3>
					<p data-id="subtitle-gauss">Assume the following functions have been given:</p>
					<pre data-id="code-gauss"><code class="hljs c++" data-trim data-line-numbers><script type="text/template">
						random_A
						zero_x
						read_b
						update_x
						compute_residual
						record_trajectory
						should_loop
						print_result
					</script></code></pre>
					<aside class="notes">
						Your task is to decide which tasks should be static, dynamic, or a branch, and to arrange them with the correct dependencies.
					</aside>
				</section>

				<section data-auto-animate data-auto-animate-id="gauss-seidel">
					<h3 data-id="title-gauss">Gauss-Seidel</h3>
					<p data-id="subtitle-gauss">Let's sketch visually first.</p>
					<aside class="notes">
						Prepare some way for the sutdents to collaborate in a drawing program.
					</aside>
				</section>

				<section data-auto-animate data-auto-animate-id="gauss-seidel">
					<h3 data-id="title-gauss">Gauss-Seidel</h3>
					<p data-id="subtitle-gauss">Now code it.</p>
					<aside class="notes">
						Run once without dependencies first, to show the thread sanitizer output.
					</aside>
				</section>


				<!-- PART VIII -->
				<!-- OTHER FEATURES -->


				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id=""></p>
					<aside class="notes">
						I this part I will quickly go through some features that Taskflow has that we might dive deeper into in a later presentation.
					</aside>
				</section>

				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id="subtitle">Composite Task Graph</p>
					<img data-id="image" src="images/composite_task.jpg">
					<aside class="notes">
						Putting  one Taskflow into another.
					</aside>
				</section>

				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id="subtitle">Asynchronous Tasks</p>
					<!--<img data-id="image" src="images/">-->
					<aside class="notes">
						Not sure what this means, exactly. Aren't all task graphs we don't wait for  asynchronous tasks?
					</aside>
				</section>

				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id="subtitle">Library of Parallel Algorithms</p>
					<p data-id="text">Such as for_each, reduce, sort.</p>
					<!--<img data-id="image" src="images/">-->
					<aside class="notes">
						Not sure how chunking works with for_each.
					</aside>
				</section>

				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id="subtitle">Pipelines</p>
					<p data-id="text">Pipe data through a series of stages.</p>
					<!--<img data-id="image" src="images/">-->
					<aside class="notes">
						Not used yet.
					</aside>
				</section>

				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id="subtitle">Executor Taskflow Management</p>
					<p data-id="text">Run a task graph asynchronously, repeated a number of times, or until a condition is satisfied.</p>
					<!--<img data-id="image" src="images/">-->
					<aside class="notes">

					</aside>
				</section>

				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id="subtitle">Heterogeneous Computing</p>
					<p data-id="text">Support for running and coordinating with GPU kernels.</p>
					<!--<img data-id="image" src="images/">-->
					<aside class="notes">

					</aside>
				</section>

				<section data-auto-animate  data-auto-animate-id="other-features">
					<h3>Other Features</h3>
					<p data-id="subtitle">Profiling</p>
					<img data-id="image" src="images/profiler.jpg">
					<aside class="notes">
						Has built-in profiling support, similar to our thread timelines. Here showing the results from a run of the Gauss-Seidel solver.
					</aside>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>