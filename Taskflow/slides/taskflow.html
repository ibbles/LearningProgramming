<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<meta name="description" conent="An introduction to Taskflow, the C++ task graph library.">
		<meta name="author" content="Martin Nilsson">

		<title>Taskflow</title>


		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">


				<!-- PART I -->
				<!-- INTRODUCTION -->
				<section>
					<h1 class="r-fit-text">Taskflow - A Task Graph Library</h1>
					<p>Subtitle?</p>
					<p>A short introduction</p>
					<p>Parallel programs made easy</p>
					<aside class="notes">
						I'm gonna be talking about computing, because I like computing.<br><br>
						This is a presentation about Taskflow, the task graph library, and why we need such a thing.
					</aside>
				</section>

				<section>
					<h3>The walls</h3>
					<img src="images/cpu-trend.jpg">
					<small><a href="https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/">https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/</a></small>

					<aside class="notes">
						For a while computer users enjoyed a steady pace of speed increases.<br><br>
						This kept going until the early 2000's when we hit the walls.<br><br>
						The power and frequency wall in particular.<br><br>
						These are joined by the so called Dennard scaling that says that as the transistors get smaller they use less energy so that the energy density, heat per area, remains roughly constant even though the number of transistors increases.<br><br>
						This stopped being the case somwhere around here, and since peak frequency is highly dependent on the voltage we can push through the transistors and heat is highly depenent on the voltage we hit a point where increasing the frequency and adding more and more tiny transistors lead to unmangeable heat generation.<br><br>
						The famous Moore's law still doing alright but single-threaded performance has not keeping up with the number of transistors and have started to level off.<br><br>
						Frequency not increasing much anymore and CPU / compiler makers are having trouble extracting more instruction level parallelism from straightline code.<br><br>
						To combat this we got more cores.<br><br>
						This means that those of us who remember the times where a new computer would make all programs run faster had to get acustomed to that not being the case anymore and we had to start working to make use all those new transistors hiding behind the second, and third, and ninetysixth core.<br><br>
						The free lunch as over, as the saying goes.
					</aside>
				</section>


				<section>
					<h3>Ways of writing multi-threaded programs</h3>
					<ul>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Manual threads</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Data parallel</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Producer / consumer</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Pipeline</p></li>
						<li><p>Task graph</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Actors / events</p></li>
						<li><p class="fragment semi-fade-out" data-fragment-index="1">Fork-join</p></li>
					</ul>
					<aside class="notes">
						Multiple cores means that we can no longer rely on single-threaded programs, we need to go parallel or concurrent.
						<br><br>
						These mean different things, but for this discussion I'm going to ignore that.
						<br><br>
						Think multi-threaded and we sort of cover what we need.
						<br><br>
						Here are some ways of organizing a multi-threaded program.
						<br><br>
						These are not clear-cut implementation strategies but broad categories.
						<br><br>
						Manual threads means starting a new thread for each task that needs to be done. Think one for audio, one for physics, one for AI, one for graphics, one for IO, and so on in a video game.
						<br><br>
						Data parallel means that we have a serial main thread and whenever we have a large set of elements we need to perform the same operation on then we bring in all the other cores to chrunch the numbers. This is how OpenMP and GPUs work. The classical parallel-for concept.
						<br><br>
						Producer / consumer can be used when we have one part of the program that through some process produces data and another part that reads that data and computes an output. If these are indenpendent then they can be run in parallel on different threads.
						<br><br>
						A pipeline can be though of a chain of producer / consumer pairs where each thread is both a consumer of its input and a producer of the output that becomes the input for the next thread.
						<br><br>
						A task graph is kinda like a branching pipeline where the output produced by one task may be consumed by multiple later tasks. In a task graph we rarely have one thread per task but a pool of worker threads that can be assigned any task that becomes ready to run. A task is typically short-lived while a pipeline is a steady-state construction where data flows through continuously.
						<br><br>
						The actor model models a system of indenpendent concurrently executing entities that listen for events and responds to them with some processing and possibly the generation of new events.
						<br><br>
						Fork-join is a broad categorization that means that that is a main flow of logic that occasionally branches out to parallel computation and that the result from those parallel computations are collected back before the main flow continues. The data parallel model is an example of fork-join.
						<br><br>
						It is possible to combine these, which is what AGX Dynamcis does.
						<br><br>
						Today the focus will be on task graphs, which forms the basis for parallelization in AGX Dynamics and what the Taskflow library provides.
						<br><br>
						Taskflow also does GPU stuff but I'm not going to go into that part today.<br><br>
					</aside>
				</section>

				<section>
					<h3>Task Graph Computing System </h3>
					<p>Instead of chaining function calls together as the program is executed the structure of the computation is defined up-front in a task graph.</p>

					<aside class="notes">
						The "up-front" part is not as rigid as it may sound may expressed like this.
					</aside>
				</section>

				<section>
					<h3>Task Based Parallelism</h3>
					<p>Construct a dependency graph with tasks as nodes.</p>
					<p>Dependencies between tasks represented as edges.</p>
					<p>Execute tasks top-to-bottom.</p>
					<p>Exploit task independence for parallelism.</p>
				</section>

				<section>
					<h3>Example Task Graph</h3>
					<img src="./images/example_task_graph.svg">
					<p>Once Task 1 has completed we can run Task 2 and 4 in parallel.</p>
				</section>

				<section>
					<h3>Static Tasking </h3>
					<p>The simplest form of task graph.</p>
					<p>All tasks are known up front.</p>
					<p>The example on the previous slide is a static task graph.</p>

					<aside class="notes">
						There are different types of task graphs with different types of expressive power.
						<br><br>
						Static tasking is the simplest form. When I said "up-front" earlier this is what I was talking about.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Fundamental Building Blocks</h3>
					<p data-id="task">A task</p>
					<p data-id="graph">A task graph</p>
					<p data-id="runtime"> A runtime</p>

					<aside class="notes">
						Let's get technical.
						<br><br>
						To realize all of this in an actual application we need three things.
						<br><br>
						Something to represent the tasks to execute. This includes both the work to be performed and metadata such as name, user data and type.
						<br><br>
						Something to represent the dependencies between tasks, the dependency graph.
						<br><br>
						A runtime that can run all tasks in the correct order.
					</aside>
				</section>

				<section data-auto-animate>
					<h3 data-id="title">Taskflow Library</h3>
					<p data-id="task">A task</p><code>tf::Task</code>
					<br><br>
					<p data-id="graph">A task graph</p> <code>tf::Taskflow</code>
					<br><br>
					<p data-id="runtime">A runtime </p> <code>tf::Executor</code>

					<aside class="notes">
						Taskflow is a C++ library and uses classes to represent these things.
						<br><br>
						All placed within the tf namespace, short for Taskflow.
						<br><br>
						Let's create and run the simplest possible task graph.
					</aside>
				</section>

				<!-- PART II -->
				<!-- SIMPLEST POSSIBLE TASK GRAPH -->

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="1-4">
						void work()
						{
							std::cout << "Doing work.\n";
						}
					</code></pre>
					<aside class="notes">
						To create a task we need to start with some work to do.
						<br><br>
						Here that work is to print the string "Doing work.".
						<br><br>
						You can imagine any other type of work one might need to do.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="6-8">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
						}
					</code></pre>
					<aside class="notes">
						With some work ready to be performed we need to do some setup to arrange for the work to be performed.
						<br><br>
						In this example we do this in main.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="8">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Taskflow taskflow;
						}
					</code></pre>
					<aside class="notes">
						We create an instance of the Taskflow class to get a task graph in which we can create our task that will call the work function.
					</aside>
				</section>


				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="9">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
						}
					</code></pre>
					<aside class="notes">
						To create a task we don't directly create an instance of the task class and add to the taskflow.
						<br><br>
						Instead we let the taskflow create the task for us by calling emplace.
						<br><br>
						The parameter to emplace is the code we want to have executed when the task is run. This can be anything that is callable with no arguments. Here we use a free function but it could just as well have been a lambda funciton or an object with a call operator.
						<br><br>
						The thing we get back is not the actual task but a handle to it. A handle that is small, cheap to pass around, and safe to copy. It's a pointer, basically. The actual data lives as a node within the task graph.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="8">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
						}
					</code></pre>
					<aside class="notes">
						With the task graph in place we are ready to run it and for this we need the runtime, which in Taskflow is called an executor.
						<br><br>
						In my example I always put it high in main to signal that this is a heavy-weigth, long-lived, and shared resource. It is not meant to be created every time we want to run a task graph. Instead we should create one at startup and reuse it throught the application's life time.
						<br><br>
						This is where the worker threads are spawned.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="11">
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
						}
					</code></pre>
					<aside class="notes">
						Now we are ready to do some work for real.
						<br><br>
						Executor.run will cause the runtime to start running tasks using the worker threads. We can chose to either keep doing other things in the main thread or wait for the tasks to finish executing. Since we have nothing else to do in this example we wait.
						<br><br>
						I have not yet found a way to make the main thread participate in the task work, which is a bit limiting given that in AGX Dynamics we always run user supplied event listeners in the main thread. There is more to Taskflow that I haven't explored yet it is possible that this can be solved.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers>
						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
						}
					</code></pre>
					<aside class="notes">
						And there you have it, the smallest possible task graph in Taskflow.
						Though we're missing some header inclues.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="1,2"><script type="text/template">
						#include "taskflow/taskflow.hpp"
						#include <iostream>

						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
						}
					</script></code></pre>
					<aside class="notes">
						This can compile and run. But we also want pretty picture, like the task graph example we saw in the introduction.
					</aside>
				</section>

				<section data-auto-animate>
					<h2 data-id="code-title">Simplest Possible Task Graph</h2>
					<pre data-id="simplest-possible-task-graph-animation"><code class="hljs c++" data-trim data-line-numbers="16-19"><script type="text/template">
						#include "taskflow/taskflow.hpp"
						#include <iostream>
						#include <fstream>

						void work()
						{
							std::cout << "Doing work.\n";
						}

						int main()
						{
							tf::Executor executor;
							tf::Taskflow taskflow;
							tf::Task task = taskflow.emplace(work);
							executor.run(taskflow).wait();
							taskflow.name("Simplest Possible Task Graph");
							task.name("A Lonely Task");
							std::ofstream file("simplest_possible_task_graph.dot");
							taskflow.dump(file);
						}
					</script></code></pre>
					<aside class="notes">
						Let's give the graph and task a name and dump them to a DOT file. It won't be all that useful for this minimal example but is a great help when build larger real-world graphs.
					</aside>
				</section>

				<section>
					<h3>SIMPLEST POSSIBLE TASK GRAPH</h3>
					<img src="./images/simplest_possible_task_graph.svg">
					<aside class="notes">
						And this is what it looks like, after some edits to get a dark-mode variant.
						<br><br>
						But sometimes we need more than one task, so let's make a new example where we create a few of them
					</aside>
				</section>

				<!-- PART III -->
				<!-- TASK DEPENDENCIES -->

				<section>
					<pre><code class="hljs c++" data-trim data-line-numbers="|2|1|3-7|3|6|"><script type="text/template">
						auto [t1, t2, t3, t4, t5, t6] = taskflow.emplace(
							[](){}, [](){}, [](){}, [](){}, [](){}, [](){});
						t1.precede(t2, t3, t4);
						t1.precede(t2, t3);
						t3.precede(t4, t5);
						t6.succeed(t2, t4, t5);
					</script></code></pre>
					<p class="fragment fade-in"><img src="./images/example_task_graph.svg"></p>
					<aside class="notes">
						From now on I won't show entire code examples since they won't fit on a slide anymore. Instead I'll show only enough to get the point accross.
						<br><br>
						I'll also use an empty lambda whenever the work done doesn't matter for the presentation. Here we have a whole bunch of them being passed to emplace.
						<br><br>
						The effect of this is that we get the same number of tasks handles back in a tuple, which we can use structured bindings to unpack into a  bunch of variables, here named t1, t2, and so on up to t6.
						<br><br>
						Dependencies are created using the precede and succeed Task member functions. precede makes the task on the left precede, i.e. happen before, the tasks on the right. succeed, on the other hand, makes the task happen after the parameter tasks.
						<br><br>
						Looking at the resuling dependency graph we get this familiar diagram from the start of the presentation.
						<br<br>
						It doesnt matter if we do t1.precede(t2) or t2.succeede(t1), both variants describe the same dependency. The only difference is that it is easy to pass many parameters to a single function so when a node has more input dependencies than output then I tend to use succeed and when it has more output dependencies thenI tend to use precede.
					</aside>
				</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>